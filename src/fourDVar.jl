# nmax: total number of integration of the model
# x of size n x (nmax+1)

function FreeRun!(‚Ñ≥,xi,Q,ùìó::AbstractModel,nmax,no,x,Hx)
    obsindex = 1;

    for n=1:nmax+1
        if n == 1
            x[:,1] = xi;
        else
            x[:,n] = ‚Ñ≥(n-1,x[:,n-1]);
        end

        if obsindex <= length(no) && n == no[obsindex]
            # extract observations
            Hx[obsindex] = ùìó(obsindex,x[:,n])
            obsindex = obsindex +1;
        end
    end
end

"""
    x,Hx = FreeRun(‚Ñ≥,xi,Q,H,nmax,no)

Performs a free-run with the model `‚Ñ≥` and `nmax` time-steps starting at the
initial condition `xi`. Observations at the time steps given in `no` are
extracted with the observation operator `H`.
"""
function FreeRun(‚Ñ≥,xi,Q,ùìó,nmax,no)
    T = eltype(xi)
    x = zeros(T,size(xi,1),nmax+1);
    Hx = Vector{Vector{T}}(undef,length(no))
    FreeRun!(‚Ñ≥,xi,Q,ùìó,nmax,no,x,Hx)
    return x,Hx
end


function costfun(xi,Pi,‚Ñ≥,xa,yo,R,ùìó,nmax,no,x,Hx)
    FreeRun!(‚Ñ≥,xa,[],ùìó,nmax,no,x,Hx);

    # cost function
    tmp = x[:,1] - xa;
    J = tmp' * (Pi \ tmp);
    for i = 1:length(no)
        tmp = yo[i] - Hx[i];
        J = J + tmp' * (R[i] \ tmp);
    end

    return J
end


function gradient(xi,dx0,x,Pi,‚Ñ≥,yo,R,ùìó,nmax,no)

    dx = zeros(size(xi,1),nmax+1);
    dx[:,1] = dx0;
    obsindex = length(no);

    for n=1:nmax
        dx[:,n+1] = tgl(‚Ñ≥,n,x[:,n],dx[:,n]);
    end

    lambda = zeros(size(xi,1),nmax+2);
    for n=nmax+1:-1:1
        lambda[:,n] = adj(‚Ñ≥,n,x[:,n],lambda[:,n+1]);

        if obsindex > 0 && n == no[obsindex]
            lambda[:,n] = lambda[:,n] +
                adj(ùìó,n,x[:,n],R[obsindex] \ (yo[obsindex] - tgl(ùìó,n,x[:,n],dx[:,n]+x[:,n])))
            obsindex = obsindex - 1;
        end
    end

    grad = 2 * (Pi \ ((dx[:,1]+x[:,1]) - xi)) - 2 * lambda[:,1];

    return grad,lambda
end

"""
    x,J = fourDVar(
            xi,Pi,‚Ñ≥,yo,R,H,nmax,no;
            innerloops = 10,
            outerloops = 2,
            tol = 1e-5)

Incremental 4D-Var with the model `‚Ñ≥` and `nmax` time-steps starting at the
initial condition `xi` and error covariance `Pi` with the specified numbers of inner
and outer loops.
Observations `yo` (and error covariance `R`) at the time steps given in `no` are
assimilated with the observation operator `H`.
"""
function fourDVar(
    xi::AbstractVector,Pi,‚Ñ≥,yo::AbstractVector,R::AbstractVector,ùìó,nmax,no;
    innerloops = 10,
    outerloops = 2,
    tol = 1e-5)

    xa = float(xi)
    T = eltype(xa)
    x = zeros(size(xi,1),nmax+1);
    Hx = Vector{Vector{T}}(undef,length(no))

    J = zeros(outerloops)
    b = zeros(size(xi))

    for i=1:outerloops

        J[i] = costfun(xi,Pi,‚Ñ≥,xa,yo,R,ùìó,nmax,no,x,Hx);

        # dx increment relative to xi

        grad(dx) = gradient(xi,dx,x,Pi,‚Ñ≥,yo,R,ùìó,nmax,no)[1];
        b .= grad(zeros(size(xi)));

        #=
        function fun(dx,fdx)
            #@show "D"
            fdx[:] = b - grad(dx)
        end

        dxa,cgsuccess,niter = DIVAnd.conjugategradient(fun,b,tol=tol/norm(b),maxit=innerloops,x0=zeros(size(xi)))
        @debug begin
            tmp = zeros(size(xi))
            fun(dxa,tmp)
            tmp = tmp - b

            @show "DIVAnd.conjugategradient",sum(tmp.^2),niter
        end
        =#

        function fg!(F,G,x)
            @debug "call fg! $(F==nothing) $(G==nothing)"
            GG = grad(x)
            if G != nothing
                # code to compute gradient here
                # writing the result to the vector G
                G .= GG
            end
            if F != nothing
                value = 0.5 * (x ‚ãÖ GG) + 0.5 * b ‚ãÖ x
                return value
            end
        end

        result = Optim.optimize(Optim.only_fg!(fg!), zeros(size(b)), ConjugateGradient(),
                                Optim.Options(g_tol = tol,
                                              iterations = innerloops,
                                              allow_f_increases=true,
                                              store_trace = true,
                                              show_trace = false))
        dxa = result.minimizer

        @debug begin
            @show summary(result)
            tmp = zeros(size(xi))
            fg!(nothing,tmp,dxa)
            @show tmp

            @show innerloops
            @show Optim.g_converged(result)
            @show Optim.f_converged(result)
            @show "optim",sum(tmp.^2),Optim.iterations(result)
        end

        # add increment to dxa
        xa .= xa + dxa;
    end

    return xa,J#,Jfun
end
